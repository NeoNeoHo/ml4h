{
 "metadata": {
  "name": "",
  "signature": "sha256:feaf9068014c895f5e5351b6673ac327d12b10e458ade5313b041a01082c7524"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import re\n",
      "import math\n",
      "import random\n",
      "import numpy as np\n",
      "import datetime as dt\n",
      "import dateutil.parser as dtp\n",
      "import matplotlib.pyplot as plt\n",
      "import textmining as txtm\n",
      "import string\n",
      "from pandas import *\n",
      "import sys\n",
      "from statsmodels.nonparametric import kde"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tdm_df(doclist, stopwords = [], remove_punctuation = True, \n",
      "           remove_digits = True, sparse_df = False):\n",
      "    '''\n",
      "    Create a term-document matrix from a list of e-mails.\n",
      "    \n",
      "    Uses the TermDocumentMatrix function in the `textmining` module.\n",
      "    But, pre-processes the documents to remove digits and punctuation,\n",
      "    and post-processes to remove stopwords, to match the functionality \n",
      "    of R's `tm` package.\n",
      "\n",
      "    NB: This is not particularly memory efficient and you can get memory \n",
      "    errors with an especially long list of documents.\n",
      "\n",
      "    Returns a (by default, sparse) DataFrame. Each column is a term,\n",
      "    each row is a document.\n",
      "    '''\n",
      "    \n",
      "    # Create the TDM from the list of documents.\n",
      "    tdm = txtm.TermDocumentMatrix()\n",
      "  \n",
      "    for doc in doclist:\n",
      "        if remove_punctuation == True:\n",
      "            doc = doc.translate(None, string.punctuation.translate(None, '\"'))\n",
      "        if remove_digits == True:\n",
      "            doc = doc.translate(None, string.digits)\n",
      "            \n",
      "        tdm.add_doc(doc)\n",
      "    \n",
      "    # Push the TDM data to a list of lists,\n",
      "    # then make that an ndarray, which then\n",
      "    # becomes a DataFrame.\n",
      "    tdm_rows = []\n",
      "    for row in tdm.rows(cutoff = 1):\n",
      "        tdm_rows.append(row)\n",
      "        \n",
      "    tdm_array = np.array(tdm_rows[1:])\n",
      "    tdm_terms = tdm_rows[0]\n",
      "    df = DataFrame(tdm_array, columns = tdm_terms)\n",
      "    \n",
      "    # Remove stopwords from the dataset, manually.\n",
      "    # TermDocumentMatrix does not do this for us.\n",
      "    if len(stopwords) > 0:\n",
      "        for col in df:\n",
      "            if col in stopwords:\n",
      "                del df[col]\n",
      "    \n",
      "    if sparse_df == True:\n",
      "        df.to_sparse(fill_value = 0)\n",
      "    \n",
      "    return df\n",
      " "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_path = os.path.abspath(os.path.join('.', 'data'))\n",
      "easyham_path = os.path.abspath(os.path.join(data_path, 'easy_ham'))\n",
      "print data_path"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/BensonHo/PycharmProjects/ipython_set/data\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_email_list(file_list):\n",
      "    for f in file_list:\n",
      "        yield parse_email(os.path.join(easyham_path, f))\n",
      "        \n",
      "def parse_email(path):\n",
      "    filename = os.path.split(path)[-1]\n",
      "    header, message = get_header_and_message(path)\n",
      "    date = get_date(header)\n",
      "    sender = get_sender(header)\n",
      "    subj = get_subject(header)\n",
      "    return date, sender, subj, message, filename\n",
      "\n",
      "def get_header_and_message(path):\n",
      "    with open(path, 'rU') as con:\n",
      "        email = con.readlines()\n",
      "        first_blank_index = email.index('\\n')\n",
      "        header = email[:first_blank_index]\n",
      "        message = ''.join(email[(first_blank_index+1):])\n",
      "    return header, message\n",
      "\n",
      "date_pattern = re.compile('^Date')\n",
      "\n",
      "def get_date(header):\n",
      "    '''\n",
      "    Find the date line of the e-mail's header, and parse\n",
      "    the date into a datetime object.\n",
      "    '''\n",
      "    # Get the first line that matches the date-line\n",
      "    # regex pattern\n",
      "    dateline = [l for l in header if \n",
      "                re.search(date_pattern, l) != None][0]\n",
      "    \n",
      "    # Grab the text after 'Date: ' (6 chars)\n",
      "    dateline = dateline[6:].strip()\n",
      "    \n",
      "    # Return the date as parsed by dateutil.parser.parse()\n",
      "    return dtp.parse(dateline)\n",
      "\n",
      "# Characters that may separate words in the From line\n",
      "splitfrom_pattern = re.compile('[\\\"\\:<> ]')\n",
      "\n",
      "def get_sender(header):\n",
      "    '''\n",
      "    Find the 'From:' line in the e-mail's header data, and \n",
      "    extract the sender's e-mail address from it.\n",
      "    '''\n",
      "    # Find line in header that contains 'From: '\n",
      "    sender = filter(lambda s: s.find('From: ') != -1, header)[0]\n",
      "    \n",
      "    # Get 'words' in From line\n",
      "    sender = re.split(splitfrom_pattern, sender)\n",
      "    sender = filter(lambda s: s != ' ' and s != '', sender)\n",
      "    \n",
      "    # Find the first word that is an e-mail address (contains @)\n",
      "    sender = filter(lambda s: '@' in s, sender)[0]\n",
      "    sender = re.sub('\\\\[a-z]', '', sender)\n",
      "    \n",
      "    return sender.lower().rstrip()\n",
      "\n",
      "def get_subject(header):\n",
      "    '''\n",
      "    Find the subject line of the e-mail's header, and extract\n",
      "    the subject text.\n",
      "    '''\n",
      "    subject = filter(lambda s: s.find('Subject: ') != -1, header)\n",
      "    if len(subject) > 0:\n",
      "        subject_start = subject[0].find('Subject: ') + 9\n",
      "        subject = subject[0][subject_start:]\n",
      "        return subject.lower()\n",
      "    else:\n",
      "        return ''\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_email_df(email_dir):\n",
      "    email_dict = {'date': [], 'sender': [],\n",
      "                  'subject': [], 'message': [], \n",
      "                  'filename': []}\n",
      "    file_list = os.listdir(email_dir)\n",
      "    file_list = [f for f in file_list if f != 'cmds']\n",
      "    \n",
      "    parsed_emails = parse_email_list(file_list)\n",
      "    \n",
      "    for pe in parsed_emails:\n",
      "        date, sender, subject, message, filename = pe\n",
      "        email_dict['date'].append(date)\n",
      "        email_dict['sender'].append(sender)\n",
      "        email_dict['subject'].append(subject)\n",
      "        email_dict['message'].append(message)\n",
      "        email_dict['filename'].append(filename)\n",
      "    email_df = DataFrame(email_dict, columns=['date', 'sender', 'subject', \n",
      "                                    'message', 'filename']) \n",
      "    return email_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "email_df = make_email_df(easyham_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print email_df.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                        date                        sender  \\\n",
        "0  2002-08-22 18:26:25+07:00             kre@munnari.oz.au   \n",
        "1  2002-08-22 12:46:18+01:00  steve.burt@cursor-system.com   \n",
        "2  2002-08-22 13:52:38+01:00                 timc@2ubh.com   \n",
        "3  2002-08-22 09:15:25-04:00              monty@roscom.com   \n",
        "4  2002-08-22 23:36:32+10:00        tony@linuxworks.com.au   \n",
        "5  2002-08-22 14:38:22+01:00     stewart.smith@ee.ed.ac.uk   \n",
        "6  2002-08-22 14:50:31+01:00      martin@srv0.ems.ed.ac.uk   \n",
        "7  2002-08-22 14:54:25+01:00      martin@srv0.ems.ed.ac.uk   \n",
        "8  2002-08-22 15:01:20+01:00     stewart.smith@ee.ed.ac.uk   \n",
        "9  2002-08-22 15:01:33+01:00      martin@srv0.ems.ed.ac.uk   \n",
        "\n",
        "                                             subject  \\\n",
        "0                         re: new sequences window\\n   \n",
        "1                        [zzzzteana] re: alexander\\n   \n",
        "2                        [zzzzteana] moscow bomber\\n   \n",
        "3            [irr] klez: the virus that  won't die\\n   \n",
        "4                             re: insert signature\\n   \n",
        "5   re: [zzzzteana] nothing like mama used to make\\n   \n",
        "6   re: [zzzzteana] nothing like mama used to make\\n   \n",
        "7  [zzzzteana] playboy wants to go out with a bang\\n   \n",
        "8   re: [zzzzteana] nothing like mama used to make\\n   \n",
        "9                 [zzzzteana] meaningful sentences\\n   \n",
        "\n",
        "                                             message  \\\n",
        "0      Date:        Wed, 21 Aug 2002 10:54:46 -05...   \n",
        "1  Martin A posted:\\nTassos Papadopoulos, the Gre...   \n",
        "2  Man Threatens Explosion In Moscow \\n\\nThursday...   \n",
        "3  Klez: The Virus That Won't Die\\n \\nAlready the...   \n",
        "4  On Wed Aug 21 2002 at 15:46, Ulises Ponce wrot...   \n",
        "5  >  in adding cream to spaghetti carbonara, whi...   \n",
        "6  \\n> I just had to jump in here as Carbonara is...   \n",
        "7  The Scotsman - 22 August 2002\\n\\n Playboy want...   \n",
        "8  Martin Adamson wrote:\\n> \\n> Isn't it just bas...   \n",
        "9  The Scotsman\\n\\n Thu 22 Aug 2002 \\n\\n Meaningf...   \n",
        "\n",
        "                                filename  \n",
        "0  0001.ea7e79d3153e7469e7a9c3e0af6a357e  \n",
        "1  0002.b3120c4bcbf3101e661161ee7efcb8bf  \n",
        "2  0003.acfc5ad94bbd27118a0d8685d18c89dd  \n",
        "3  0004.e8d5727378ddde5c3be181df593f1712  \n",
        "4  0005.8c3b9e9c0f3f183ddaf7592a11b99957  \n",
        "5  0006.ee8b0dba12856155222be180ba122058  \n",
        "6  0007.c75188382f64b090022fa3b095b020b0  \n",
        "7  0008.20bc0b4ba2d99aae1c7098069f611a9b  \n",
        "8  0009.435ae292d75abb1ca492dcc2d5cf1570  \n",
        "9  0010.4996141de3f21e858c22f88231a9f463  \n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}